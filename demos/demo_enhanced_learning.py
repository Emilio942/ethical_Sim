#!/usr/bin/env python3
"""
Enhanced Demo f√ºr die ethische Agenten-Simulation mit erweiterten Lernmechanismen.

Zeigt die neuen Features:
- Reinforcement Learning
- Multi-Criteria Decision Making  
- Uncertainty Handling
- Advanced Social Learning
- Temporal Dynamics
- Metacognitive Monitoring
"""

import numpy as np
import random
from typing import List, Dict
from agents import NeuralEthicalAgent
from beliefs import NeuralEthicalBelief
from scenarios import ScenarioGenerator
from neural_society import NeuralEthicalSociety

def create_enhanced_agent(agent_id: str, agent_type: str = "balanced") -> NeuralEthicalAgent:
    """Erstellt einen Agenten mit spezifischen Eigenschaften f√ºr die Enhanced Demo."""
    
    if agent_type == "systematic":
        traits = {
            "openness": 0.9,
            "conscientiousness": 0.9, 
            "extroversion": 0.5,
            "agreeableness": 0.6,
            "neuroticism": 0.3
        }
    elif agent_type == "intuitive":
        traits = {
            "openness": 0.7,
            "conscientiousness": 0.4,
            "extroversion": 0.8,
            "agreeableness": 0.7,
            "neuroticism": 0.6
        }
    elif agent_type == "social":
        traits = {
            "openness": 0.6,
            "conscientiousness": 0.6,
            "extroversion": 0.9,
            "agreeableness": 0.9,
            "neuroticism": 0.4
        }
    else:  # balanced
        traits = {
            "openness": 0.6,
            "conscientiousness": 0.6,
            "extroversion": 0.6,
            "agreeableness": 0.6,
            "neuroticism": 0.5
        }
    
    agent = NeuralEthicalAgent(agent_id, traits)
    
    # Standard-√úberzeugungen hinzuf√ºgen
    agent.add_belief(NeuralEthicalBelief('Utilitarismus', np.random.uniform(0.3, 0.9), np.random.uniform(0.6, 0.9)))
    agent.add_belief(NeuralEthicalBelief('Deontologie', np.random.uniform(0.3, 0.9), np.random.uniform(0.6, 0.9)))
    agent.add_belief(NeuralEthicalBelief('Tugendethik', np.random.uniform(0.3, 0.9), np.random.uniform(0.6, 0.9)))
    
    return agent

def demo_reinforcement_learning():
    """Demonstriert Reinforcement Learning Mechanismen."""
    print("=" * 60)
    print("üß† REINFORCEMENT LEARNING DEMO")
    print("=" * 60)
    
    agent = create_enhanced_agent("rl_learner", "systematic")
    generator = ScenarioGenerator()
    
    print(f"Agent erstellt: {agent.agent_id}")
    print(f"Initial exploration rate: {agent.rl_system['exploration_rate']:.3f}")
    print(f"Initial learning rate: {agent.rl_system['learning_rate']:.3f}")
    
    # Mehrere Entscheidungen in √§hnlichen Szenarien
    for round_num in range(5):
        print(f"\n--- Runde {round_num + 1} ---")
        
        scenario = generator.create_scenario_from_template('trolley_problem')
        decision = agent.make_decision(scenario, enhanced=True)
        
        print(f"Entscheidung: {decision['chosen_option']}")
        print(f"Konfidenz: {decision['confidence']:.3f}")
        print(f"Exploration verwendet: {decision['exploration_used']}")
        
        # Simuliere Feedback (zuf√§llig f√ºr Demo)
        outcome_feedback = {
            'personal_outcome': np.random.uniform(-0.5, 0.8),
            'social_outcome': np.random.uniform(-0.3, 0.9),
            'moral_outcome': np.random.uniform(-0.2, 0.7),
            'risk_outcome': np.random.uniform(-0.8, 0.3)
        }
        
        learning_result = agent.learn_from_decision_outcome(scenario, decision['chosen_option'], outcome_feedback)
        
        print(f"RL Belohnung: {learning_result['reinforcement_learning']['total_reward']:.3f}")
        print(f"Neue exploration rate: {agent.rl_system['exploration_rate']:.3f}")
        
        # Zeige gelernte Aktionswerte
        if "trolley" in agent.rl_system["action_values"]:
            trolley_values = agent.rl_system["action_values"]["trolley"]
            print(f"Gelernte Werte: {trolley_values}")

def demo_multi_criteria_decision():
    """Demonstriert Multi-Criteria Decision Making."""
    print("\n" + "=" * 60)
    print("‚öñÔ∏è  MULTI-CRITERIA DECISION MAKING DEMO")
    print("=" * 60)
    
    agent = create_enhanced_agent("mc_agent", "balanced")
    generator = ScenarioGenerator()
    
    print(f"Agent erstellt: {agent.agent_id}")
    print(f"Initial criteria weights: {agent.decision_criteria['utility_weights']}")
    
    scenario = generator.create_scenario_from_template('autonomous_vehicle')
    decision = agent.make_decision(scenario, enhanced=True)
    
    print(f"\nSzenario: {scenario.scenario_id}")
    print(f"Entscheidung: {decision['chosen_option']}")
    print(f"Konfidenz: {decision['confidence']:.3f}")
    
    # Detaillierte Kriterien-Analyse
    if 'criteria_evaluations' in decision:
        print("\nKriterien-Bewertungen:")
        for option, criteria in decision['criteria_evaluations'].items():
            print(f"  {option}:")
            for criterion, score in criteria.items():
                print(f"    {criterion}: {score:.3f}")
    
    print(f"\nFinale Option-Scores: {decision['option_scores']}")
    
    # Lerne aus Feedback und zeige Gewichts-Anpassungen
    outcome_feedback = {
        'personal_outcome': 0.4,
        'social_outcome': 0.8,
        'moral_outcome': 0.6,
        'risk_outcome': 0.1
    }
    
    learning_result = agent.learn_from_decision_outcome(scenario, decision['chosen_option'], outcome_feedback)
    
    print(f"\nNach dem Lernen:")
    print(f"Neue criteria weights: {agent.decision_criteria['utility_weights']}")
    if 'criteria_learning' in learning_result:
        print(f"Gewichts-√Ñnderungen: {learning_result['criteria_learning']}")

def demo_uncertainty_handling():
    """Demonstriert Uncertainty und Ambiguity Handling."""
    print("\n" + "=" * 60)
    print("ü§î UNCERTAINTY HANDLING DEMO")
    print("=" * 60)
    
    # Agent mit hoher Ambiguity Tolerance
    tolerant_agent = create_enhanced_agent("tolerant_agent", "intuitive")
    tolerant_agent.uncertainty_handling["ambiguity_tolerance"] = 0.9
    tolerant_agent.uncertainty_handling["epistemic_humility"] = 0.8
    
    # Agent mit niedriger Ambiguity Tolerance  
    intolerant_agent = create_enhanced_agent("intolerant_agent", "systematic")
    intolerant_agent.uncertainty_handling["ambiguity_tolerance"] = 0.2
    intolerant_agent.uncertainty_handling["epistemic_humility"] = 0.3
    
    generator = ScenarioGenerator()
    scenario = generator.create_scenario_from_template('privacy_vs_security')
    
    print(f"Szenario: {scenario.scenario_id}")
    
    for agent, label in [(tolerant_agent, "Unsicherheits-Tolerant"), (intolerant_agent, "Unsicherheits-Intolerant")]:
        print(f"\n--- {label} Agent ---")
        print(f"Ambiguity tolerance: {agent.uncertainty_handling['ambiguity_tolerance']:.3f}")
        
        decision = agent.make_decision(scenario, enhanced=True)
        
        print(f"Entscheidung: {decision['chosen_option']}")
        print(f"Konfidenz: {decision['confidence']:.3f}")
        
        if 'uncertainty_info' in decision:
            print(f"Unsicherheitssch√§tzungen: {len(decision['uncertainty_info'])} √úberzeugungen")
            avg_uncertainty = np.mean(list(decision['uncertainty_info'].values())) if decision['uncertainty_info'] else 0
            print(f"Durchschnittliche Unsicherheit: {avg_uncertainty:.3f}")

def demo_social_learning():
    """Demonstriert Advanced Social Learning."""
    print("\n" + "=" * 60)
    print("üë• SOCIAL LEARNING DEMO")
    print("=" * 60)
    
    # Erstelle eine kleine Gesellschaft mit verschiedenen Agenten-Typen
    agents = [
        create_enhanced_agent("social_1", "social"),
        create_enhanced_agent("systematic_1", "systematic"), 
        create_enhanced_agent("intuitive_1", "intuitive"),
        create_enhanced_agent("balanced_1", "balanced")
    ]
    
    # Soziale Verbindungen erstellen
    for i, agent in enumerate(agents):
        for j, other_agent in enumerate(agents):
            if i != j:
                # Zuf√§llige Verbindungsst√§rke
                connection_strength = np.random.uniform(0.3, 0.9)
                agent.add_social_connection(other_agent.agent_id, connection_strength)
    
    # Fokus-Agent f√ºr detaillierte Analyse
    focus_agent = agents[0]
    other_agents = agents[1:]
    
    print(f"Focus Agent: {focus_agent.agent_id}")
    print(f"Soziale Verbindungen: {focus_agent.social_connections}")
    
    # Vor sozialem Lernen
    print(f"\nVor sozialem Lernen:")
    for belief_name, belief in focus_agent.beliefs.items():
        print(f"  {belief_name}: {belief.strength:.3f}")
    
    # Soziales Lernen durchf√ºhren
    social_changes = focus_agent.advanced_social_learning(other_agents)
    
    print(f"\nNach sozialem Lernen:")
    for belief_name, belief in focus_agent.beliefs.items():
        print(f"  {belief_name}: {belief.strength:.3f}")
    
    print(f"\nSoziale √Ñnderungen: {social_changes}")
    print(f"Trust Network: {focus_agent.social_learning['trust_network']}")

def demo_temporal_dynamics():
    """Demonstriert Temporal Belief Dynamics."""
    print("\n" + "=" * 60)
    print("‚è∞ TEMPORAL DYNAMICS DEMO")
    print("=" * 60)
    
    agent = create_enhanced_agent("temporal_agent", "balanced")
    
    print(f"Agent erstellt: {agent.agent_id}")
    
    # Verfolge eine spezifische √úberzeugung √ºber Zeit
    belief_name = "Utilitarismus"
    initial_strength = agent.beliefs[belief_name].strength
    
    print(f"\nInitiale St√§rke von {belief_name}: {initial_strength:.3f}")
    
    # Simuliere zeitliche Entwicklung
    strengths_over_time = [initial_strength]
    
    for time_step in range(10):
        # F√ºge Momentum hinzu (simuliert anhaltende Einfl√ºsse)
        if time_step == 3:
            agent.temporal_dynamics["belief_momentum"][belief_name] = 0.2
            print(f"  Zeit {time_step}: Momentum hinzugef√ºgt")
        
        if time_step == 7:
            agent.temporal_dynamics["habituation_effects"][belief_name] = 0.8
            print(f"  Zeit {time_step}: Habituation-Effekt")
        
        # Zeitliche Dynamik anwenden
        temporal_changes = agent.temporal_belief_dynamics(1.0)
        
        current_strength = agent.beliefs[belief_name].strength
        strengths_over_time.append(current_strength)
        
        if belief_name in temporal_changes:
            change = temporal_changes[belief_name]
            print(f"  Zeit {time_step + 1}: {current_strength:.3f} (Œî: {change:.3f})")
    
    print(f"\nSt√§rke-Verlauf: {[f'{s:.3f}' for s in strengths_over_time[:5]]}...")

def demo_metacognitive_monitoring():
    """Demonstriert Metacognitive Monitoring."""
    print("\n" + "=" * 60)
    print("üßê METACOGNITIVE MONITORING DEMO")
    print("=" * 60)
    
    # Agent mit hoher metacognitiver Bewusstheit
    agent = create_enhanced_agent("meta_agent", "systematic")
    agent.metacognition["thinking_about_thinking"] = True
    agent.metacognition["strategy_monitoring"] = 0.9
    agent.metacognition["error_detection"] = 0.8
    
    print(f"Agent erstellt: {agent.agent_id}")
    print(f"Metacognitive features: {agent.metacognition}")
    
    generator = ScenarioGenerator()
    
    # Simuliere eine Sequenz von Entscheidungen mit unterschiedlichen Ergebnissen
    scenarios = ['trolley_problem', 'autonomous_vehicle', 'privacy_vs_security']
    
    for i, template in enumerate(scenarios):
        print(f"\n--- Entscheidung {i + 1}: {template} ---")
        
        scenario = generator.create_scenario_from_template(template)
        decision = agent.make_decision(scenario, enhanced=True)
        
        print(f"Entscheidung: {decision['chosen_option']}")
        print(f"Konfidenz: {decision['confidence']:.3f}")
        print(f"Kognitive Dissonanz: {decision['cognitive_dissonance']:.3f}")
        
        # Simuliere unterschiedliche Qualit√§t der Ergebnisse
        if i == 0:  # Gutes Ergebnis
            outcome_feedback = {
                'personal_outcome': 0.8,
                'social_outcome': 0.9,
                'moral_outcome': 0.7,
                'risk_outcome': 0.2
            }
        elif i == 1:  # Schlechtes Ergebnis
            outcome_feedback = {
                'personal_outcome': -0.3,
                'social_outcome': -0.5,
                'moral_outcome': -0.2,
                'risk_outcome': -0.8
            }
        else:  # Mittleres Ergebnis
            outcome_feedback = {
                'personal_outcome': 0.1,
                'social_outcome': 0.2,
                'moral_outcome': 0.3,
                'risk_outcome': 0.0
            }
        
        learning_result = agent.learn_from_decision_outcome(scenario, decision['chosen_option'], outcome_feedback)
        
        if 'metacognitive_adjustments' in decision:
            print(f"Metacognitive Anpassungen: {decision['metacognitive_adjustments']}")
        
        if 'meta_learning' in learning_result:
            meta_info = learning_result['meta_learning']
            print(f"Meta-Learning Effektivit√§t: {meta_info['effectiveness']:.3f}")
            print(f"Angepasste Lernrate: {meta_info['learning_rate_adjustment']:.3f}")

def main():
    """Hauptdemo f√ºr erweiterte Lernmechanismen."""
    print("üöÄ ENHANCED LEARNING MECHANISMS DEMO")
    print("Demonstriert alle neuen fortgeschrittenen Lernf√§higkeiten der ethischen Agenten")
    
    try:
        demo_reinforcement_learning()
        demo_multi_criteria_decision()
        demo_uncertainty_handling()
        demo_social_learning()
        demo_temporal_dynamics()
        demo_metacognitive_monitoring()
        
        print("\n" + "=" * 60)
        print("‚úÖ ALLE ENHANCED LEARNING DEMOS ERFOLGREICH ABGESCHLOSSEN!")
        print("=" * 60)
        print("\nDie erweiterten Lernmechanismen umfassen:")
        print("‚Ä¢ Reinforcement Learning mit Exploration/Exploitation")
        print("‚Ä¢ Multi-Criteria Decision Making mit Trade-off Analysis")
        print("‚Ä¢ Uncertainty und Ambiguity Handling")
        print("‚Ä¢ Advanced Social Learning mit Trust und Reputation")
        print("‚Ä¢ Temporal Belief Dynamics mit Momentum und Habituation")
        print("‚Ä¢ Metacognitive Monitoring und Strategy Switching")
        
    except Exception as e:
        print(f"‚ùå Fehler in der Enhanced Demo: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
